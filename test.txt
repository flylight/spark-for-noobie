Spark actually comes bundled with a "cluster resource manager" which can divide and share the physical resources of a cluster of machines between multiple Spark applications. So there are essentially two parts - 

1. The core Spark functionality that can create and process RDDs. This part can use any of the common "cluster resource managers" (YARN, Mesos, Spark Standalone) to actually schedule and run tasks. 

2. Spark's Standalone cluster manager @Spark Standalone Mode  which is a cluster manager that can be used to run Spark applications on a cluster, similar to YARN and Mesos.

The way Spark application and Spark standalone cluster manager operates is as follows. The machine where the Spark application process (the one that creates the SparkContext) is running is the "Driver" node, with process being called the Driver process. There is another machine where the Spark Standalone cluster manager is running, called the "Master" node. Along side, in each of the machines in the cluster, there is a "Worker" process running which reports the available resources in its node to the "Master". 

When the Driver process needs resources to run jobs/tasks, it ask the "Master" for some resources. The "Master" allocates the resources and uses the "Workers" running through out the cluster to create "Executors" for the "Driver". Then the Driver can run tasks in those "Executors".